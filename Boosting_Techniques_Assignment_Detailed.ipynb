{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgJ57kn8AiNc"
      },
      "source": [
        "# Boosting Techniques Assignment (DA-AG-015)\n",
        "### Detailed Solutions with Original Questions\n",
        "---\n"
      ],
      "id": "OgJ57kn8AiNc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyp0mtIcAiNh"
      },
      "source": [
        "## Question 1\n",
        "**What is Boosting in Machine Learning? Explain how it improves weak learners.**"
      ],
      "id": "Nyp0mtIcAiNh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BagtWb5_AiNi"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Boosting is an ensemble technique that combines multiple weak learners (usually shallow decision trees) into a strong learner. It works sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
        "\n",
        "**How it improves weak learners:**\n",
        "- Assigns higher weights to misclassified instances.\n",
        "- Sequentially learns from mistakes.\n",
        "- Aggregates weak models into a strong classifier.\n",
        "\n",
        "Examples: AdaBoost, Gradient Boosting, XGBoost, CatBoost."
      ],
      "id": "BagtWb5_AiNi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyei_OrtAiNi"
      },
      "source": [
        "## Question 2\n",
        "**What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**"
      ],
      "id": "zyei_OrtAiNi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3fgid1AiNi"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "| Aspect | AdaBoost | Gradient Boosting |\n",
        "|--------|----------|------------------|\n",
        "| Training | Sequential re-weighting of data points | Sequential minimization of loss function via gradients |\n",
        "| Focus | Misclassified samples get higher weight | Fits to residual errors using gradient descent |\n",
        "| Loss | Exponential loss | Any differentiable loss (e.g., MSE, log-loss) |\n",
        "| Base learner | Decision Stump (shallow tree) | Deeper decision trees |\n",
        "\n",
        "AdaBoost adjusts sample weights, while Gradient Boosting uses gradients to minimize loss."
      ],
      "id": "gk3fgid1AiNi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPb62EpxAiNj"
      },
      "source": [
        "## Question 3\n",
        "**How does regularization help in XGBoost?**"
      ],
      "id": "DPb62EpxAiNj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9H4Wn6dAiNj"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Regularization in XGBoost helps prevent overfitting and improves generalization by penalizing model complexity.\n",
        "\n",
        "- **L1 (Lasso) penalty:** Encourages sparsity, feature selection.\n",
        "- **L2 (Ridge) penalty:** Smoothens weights, prevents overfitting.\n",
        "- **Tree-specific regularization:** Parameters like `max_depth`, `min_child_weight`, `subsample` control complexity.\n",
        "\n",
        "This ensures that models remain robust and not overly complex."
      ],
      "id": "g9H4Wn6dAiNj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wENiFbjAiNj"
      },
      "source": [
        "## Question 4\n",
        "**Why is CatBoost considered efficient for handling categorical data?**"
      ],
      "id": "0wENiFbjAiNj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3u41KQ2AiNk"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "CatBoost is efficient for categorical data because:\n",
        "- Uses **ordered boosting** to prevent target leakage.\n",
        "- Automatically handles categorical variables without one-hot encoding.\n",
        "- Converts categories into numerical values using target statistics.\n",
        "- Reduces preprocessing effort and improves accuracy on categorical-rich datasets."
      ],
      "id": "G3u41KQ2AiNk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMw2BY3CAiNk"
      },
      "source": [
        "## Question 5\n",
        "**What are some real-world applications where boosting techniques are preferred over bagging methods?**"
      ],
      "id": "yMw2BY3CAiNk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnh70-RfAiNk"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Boosting is often preferred when accuracy is critical and the dataset has complex patterns. Examples:\n",
        "- **Finance:** Loan default prediction, fraud detection.\n",
        "- **Healthcare:** Disease prediction, drug response modeling.\n",
        "- **Marketing:** Customer churn prediction, recommendation systems.\n",
        "- **Cybersecurity:** Intrusion detection, malware classification.\n",
        "\n",
        "Boosting excels in high-stakes, imbalanced classification problems."
      ],
      "id": "Vnh70-RfAiNk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bddv0-0QAiNk"
      },
      "source": [
        "## Question 6\n",
        "**Write a Python program to:**\n",
        "- Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "- Print the model accuracy"
      ],
      "id": "bddv0-0QAiNk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwP4JfNzAiNl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost\n",
        "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "y_pred = ada.predict(X_test)\n",
        "\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "id": "fwP4JfNzAiNl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RzijSQ2AiNm"
      },
      "source": [
        "## Question 7\n",
        "**Write a Python program to:**\n",
        "- Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "- Evaluate performance using R-squared score"
      ],
      "id": "1RzijSQ2AiNm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyVbH6H5AiNm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n"
      ],
      "id": "fyVbH6H5AiNm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAqT8DXzAiNm"
      },
      "source": [
        "## Question 8\n",
        "**Write a Python program to:**\n",
        "- Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "- Tune the learning rate using GridSearchCV\n",
        "- Print the best parameters and accuracy"
      ],
      "id": "FAqT8DXzAiNm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-F-9TNFAiNn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "grid = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "id": "t-F-9TNFAiNn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc7bjFaSAiNn"
      },
      "source": [
        "## Question 9\n",
        "**Write a Python program to:**\n",
        "- Train a CatBoost Classifier\n",
        "- Plot the confusion matrix using seaborn"
      ],
      "id": "sc7bjFaSAiNn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HIiUnhTAiNo"
      },
      "outputs": [],
      "source": [
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Train CatBoost\n",
        "cat = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "cat.fit(X_train, y_train)\n",
        "y_pred = cat.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "id": "9HIiUnhTAiNo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHji80OIAiNo"
      },
      "source": [
        "## Question 10\n",
        "**Case Study:**\n",
        "You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "**Step-by-step Approach:**\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "   - Handle missing values (imputation).\n",
        "   - Encode categorical variables (CatBoost handles them natively).\n",
        "   - Normalize/scale numeric features if needed.\n",
        "\n",
        "2. **Choice of Boosting Method:**\n",
        "   - CatBoost is preferred (handles categorical + missing values well).\n",
        "   - XGBoost is also effective but needs preprocessing.\n",
        "\n",
        "3. **Hyperparameter Tuning:**\n",
        "   - Use GridSearchCV or RandomizedSearchCV.\n",
        "   - Parameters: learning_rate, max_depth, n_estimators, subsample.\n",
        "\n",
        "4. **Evaluation Metrics:**\n",
        "   - AUC-ROC (captures performance on imbalanced data).\n",
        "   - Precision-Recall (important to reduce false negatives).\n",
        "\n",
        "5. **Business Benefits:**\n",
        "   - Reduce risk by identifying high-risk borrowers.\n",
        "   - Improve profitability by minimizing loan defaults.\n",
        "   - Better customer segmentation and decision-making.\n"
      ],
      "id": "eHji80OIAiNo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}