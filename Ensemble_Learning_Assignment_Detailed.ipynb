{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RushikeshChathe/FUNCTIONS/blob/main/Ensemble_Learning_Assignment_Detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Murpjo5P9O3g"
      },
      "source": [
        "# Ensemble Learning Assignment (DA-AG-014)\n",
        "### Detailed Solutions with Original Questions\n",
        "---\n"
      ],
      "id": "Murpjo5P9O3g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJhH32He9O3j"
      },
      "source": [
        "## Question 1\n",
        "**What is Ensemble Learning in machine learning? Explain the key idea behind it.**"
      ],
      "id": "rJhH32He9O3j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMykenkU9O3k"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Ensemble Learning is a machine learning technique where multiple models (often weak learners) are combined to improve prediction performance. The key idea is that while individual models may suffer from high variance or bias, their combination balances errors and produces better accuracy, stability, and generalization.\n",
        "\n",
        "- **Why use ensembles?**\n",
        "  - Reduce variance (bagging)\n",
        "  - Reduce bias (boosting)\n",
        "  - Improve robustness (stacking)\n",
        "\n",
        "**Example:** Random Forest combines many Decision Trees using bagging, which results in better accuracy than a single tree."
      ],
      "id": "dMykenkU9O3k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuitQKpt9O3k"
      },
      "source": [
        "## Question 2\n",
        "**What is the difference between Bagging and Boosting?**"
      ],
      "id": "AuitQKpt9O3k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvTMCm0D9O3k"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "| Aspect | Bagging | Boosting |\n",
        "|--------|---------|----------|\n",
        "| Training | Models trained in parallel | Models trained sequentially |\n",
        "| Focus | Reduce variance | Reduce bias |\n",
        "| Sampling | Bootstrap sampling (random with replacement) | Weighted sampling (focus on errors) |\n",
        "| Model weights | Equal contribution | Higher weight to better learners |\n",
        "| Examples | Random Forest | AdaBoost, Gradient Boosting, XGBoost |\n",
        "\n",
        "**Summary:** Bagging stabilizes predictions, while Boosting makes weak models stronger."
      ],
      "id": "wvTMCm0D9O3k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-fm7Can9O3l"
      },
      "source": [
        "## Question 3\n",
        "**What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**"
      ],
      "id": "S-fm7Can9O3l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK5TDvJa9O3l"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Bootstrap sampling creates training datasets by sampling with replacement from the original dataset. Each bootstrap sample is of the same size but includes duplicates.\n",
        "\n",
        "**Role in Bagging (e.g., Random Forest):**\n",
        "- Each tree is trained on a different bootstrap sample.\n",
        "- Introduces diversity among models.\n",
        "- Reduces overfitting and variance.\n",
        "\n",
        "Thus, bootstrap sampling ensures that no two trees are identical."
      ],
      "id": "vK5TDvJa9O3l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3QBhXs69O3l"
      },
      "source": [
        "## Question 4\n",
        "**What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**"
      ],
      "id": "f3QBhXs69O3l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSME57ON9O3m"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- **OOB samples:** Data points not selected in a bootstrap sample (~37% of data).\n",
        "- **OOB score:** Predictions for each data point are made only by the trees where it was OOB. Aggregating these gives an unbiased performance estimate without needing a separate validation set."
      ],
      "id": "SSME57ON9O3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVIQGHR99O3m"
      },
      "source": [
        "## Question 5\n",
        "**Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**"
      ],
      "id": "ZVIQGHR99O3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g08j-DaY9O3m"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- **Decision Tree:** Feature importance is based on impurity reduction (Gini/Entropy). Can be unstable and biased towards features with more categories.\n",
        "- **Random Forest:** Aggregates importance across many trees â†’ more reliable, robust, and unbiased."
      ],
      "id": "g08j-DaY9O3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYd2az59O3n"
      },
      "source": [
        "## Question 6\n",
        "**Write a Python program to:**\n",
        "- Load the Breast Cancer dataset using `sklearn.datasets.load_breast_cancer()`\n",
        "- Train a Random Forest Classifier\n",
        "- Print the top 5 most important features based on feature importance scores."
      ],
      "id": "fQYd2az59O3n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgttKf3z9O3n"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importances = pd.Series(rf.feature_importances_, index=feature_names)\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5)\n"
      ],
      "id": "tgttKf3z9O3n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXLyT57u9O3o"
      },
      "source": [
        "## Question 7\n",
        "**Write a Python program to:**\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "id": "mXLyT57u9O3o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeb63TUq9O3o"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "id": "eeb63TUq9O3o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LCGUY9w9O3o"
      },
      "source": [
        "## Question 8\n",
        "**Write a Python program to:**\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters `max_depth` and `n_estimators` using GridSearchCV\n",
        "- Print the best parameters and final accuracy"
      ],
      "id": "1LCGUY9w9O3o"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZIO7XuSP9O3p",
        "outputId": "cf919b7d-21f1-4bb1-8523-b41534c211e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4249600199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reuse Breast Cancer dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m param_grid = {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Reuse Breast Cancer dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "id": "ZIO7XuSP9O3p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvQbMsGm9O3p"
      },
      "source": [
        "## Question 9\n",
        "**Write a Python program to:**\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)"
      ],
      "id": "DvQbMsGm9O3p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk4bUzSN9O3p"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "id": "Xk4bUzSN9O3p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Z0joeH9O3p"
      },
      "source": [
        "## Question 10\n",
        "**Case Study:**\n",
        "You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "**You decide to use ensemble techniques to increase model performance.**\n",
        "\n",
        "**Step-by-step Approach:**\n",
        "\n",
        "1. **Choose between Bagging or Boosting:**\n",
        "   - Loan default prediction is an imbalanced classification problem.\n",
        "   - Boosting (XGBoost, LightGBM) is preferred because it handles imbalance and reduces bias.\n",
        "\n",
        "2. **Handle overfitting:**\n",
        "   - Use cross-validation and early stopping.\n",
        "   - Regularization parameters (lambda, alpha).\n",
        "   - Control depth of trees (max_depth).\n",
        "\n",
        "3. **Select base models:**\n",
        "   - Decision Trees as weak learners.\n",
        "   - Combine them with Boosting algorithms.\n",
        "\n",
        "4. **Evaluate performance:**\n",
        "   - Use Stratified k-Fold cross-validation.\n",
        "   - Metrics: AUC-ROC, Precision-Recall, F1-score (important for imbalanced datasets).\n",
        "\n",
        "5. **Justification:**\n",
        "   - Ensemble learning captures nonlinear patterns in financial data.\n",
        "   - Boosting improves accuracy on minority class (loan defaults).\n",
        "   - Reduces business risk by minimizing false negatives (approving risky loans).\n"
      ],
      "id": "q1Z0joeH9O3p"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}